\chapter{\IfLanguageName{dutch}{Stand van zaken}{State of the art}}%
\label{ch:stand-van-zaken}

% Tip: Begin elk hoofdstuk met een paragraaf inleiding die beschrijft hoe
% dit hoofdstuk past binnen het geheel van de bachelorproef. Geef in het
% bijzonder aan wat de link is met het vorige en volgende hoofdstuk.

% Pas na deze inleidende paragraaf komt de eerste sectiehoofding.
\section{IMIS}
Het Integrated Marine Information System (IMIS) \autocite{Haspeslagh2024} heeft als doel het mariene onderzoek in Vlaanderen te coördineren. Die rol wordt vervuld door op te treden als centraal kenniscentrum voor en door de mariene sector in Vlaanderen. De taken van IMIS gaan verder dan louter het uitbouwen van collecties met publicaties, ze omvatten ook referenties naar wetenschappers en naar projecten.
\section{Google Scholar}
Google Scholar (GS) is een online index van academische literatuur. De beta-versie verscheen in 2004 en sindsdien wordt het systeem voornamelijk door academici gebruikt om een persoonlijke bibliotheek aan te leggen tezamen met statistieken omtrent citaties en h-indexen \footnote{De h-index van een wetenschappelijk onderzoeker komt overeen met de grootste h van het aantal publicaties die minstens h keer geciteerd zijn in ander werk. vb. Een h-index van 3 betekent dat de auteur minstens 3 publicaties heeft die elk minstens 3 keer geciteerd werden.}. GS laat toe om te gaan zoeken op sleutelwoorden, titel, auteur, domein en combinaties van deze \autocite{Noruzi2005}.\\
Er zijn verschillende kenmerken die bijdragen aan het succes van GS ten opzichte van de andere grote indexen. GS is beschikbaar zonder kosten, het bevat de grootste bibliografische collectie ter wereld, en de data zijn afkomstig van zowel publieke als niet publieke bronnen \autocite{Aguillo2011}.
\section{Web scraping}
Web scraping of web crawling is een verzamelnaam voor een groep technieken waarmee gegevens onttrokken kunnen worden van webpagina's \autocite{Bhatt2023}. Web scrapers kunnen gebaseerd zijn op verschillende technologieën zoals 'spidering' en 'pattern matching'. \textcite{Lotfi2021} onderscheidt web scrapers zowel op basis van hun toepassing als op basis van hun  methodiek. Daarnaast biedt de programmeertaal waarin ze geïmplenteerd zijn ook telkens andere mogelijkheden \autocite{Bhatt2023}.
Python lijkt de voor de hand liggende taal voor web scraping door de beschikbaarheid van een groot aantal libraries voor het beheren van data \autocite{Kumar2023}.\\
Met de opkomst van AI blijkt het ook mogelijk om een webpagina te scrapen met gebruik van een Large Language Model (LLM) \autocite{Ahluwalia2024}. AI biedt voordelen voor het omgaan met dynamische websites in vergelijking met de klassieke methodes en verbetert de efficiëntie van het scrapen \autocite{Ayuso2024}.\\
Web scraping vergroot de snelheid en het volume van de data die verwerkt kunnen worden aanzienlijk, en vermindert ook het aantal fouten die zouden optreden door menselijke verwerking \autocite{Bhatt2023}. Uiteindelijk zet web scraping de online informatie om in business intelligence afhankelijk van het uitgangspunt van de eindgebruiker.\\
\textcite{Pratiba2018},\textcite{Rafsanjani2022},\textcite{Amin2024},\textcite{Sulistya2024} scrapen onafhankelijk van elkaar GS aan de hand van het parsen van de DOM \footnote{Document Object Model}.\\
Voorbeelden die gebruik maken van AI, hebben nog niet tot een publicatie geleid, maar blijven beperkt tot blog posts (\autocite{Serpapiai2025}, \autocite{Anthropic2025}).\\
\textcite{Yang2017} beschrijft welke HTML elementen belangrijk zijn bij het ontleden van een GS pagina en \textcite{Rahmatulloh2020} toont hoe deze kunnen gemapt worden naar de custom code van de scraper.\\
Los van de manier waarop, blijft het doel steevast de omzetting van ongestructureerde gegevens op webpagina's naar gestructureerde data in een databank \autocite{Singrodia2019}.\\
\textcite{Mitchell2015} reikt een aantal methodes aan waarmee dit gedaan kan worden, steeds afhankelijk van het beoogde resultaat. De skills om al die data te beheren en ermee te interageren is zo mogelijk nog belangrijk dan het scrapen op zich. Gescrapete data zijn niet gekenmerkt door strikte relaties, maar stellen eerder een collectie semi gestructureerde data voor. Een document store database lijkt in dit geval het meest aangewezen omdat er geen schema voor nodig is maar de data toch gestructureerd opgeslagen kunnen worden \autocite{Meier2019}. \textcite{Lourenco2015} vergelijkt de gangbare NoSQL databases en stelt MongoDB voor als document store database.\\\\
Het is belangrijk even stil te staan bij de vraag of web scraping legaal is, aangezien er constant met data van anderen gewerkt wordt? Volgens \textcite{EPSI2015} is daar geen eenvoudig antwoord op, omdat de situatie afhankelijk is van het betrokken land maar over het algemeen zijn de meeste reguleringen wel in het voordeel van web scraping. Men moet vooral opletten met het respecteren van het eigendomsrecht wanneer inhoud verworven wordt door scraping en vervolgens verder gebruikt wordt.\\\\
Tenslotte is het interessant om te beseffen dat een groot deel van de informatie op het internet gepresenteerd wordt onder de vorm van PDF-documenten \autocite{Singrodia2019}. Dit geeft aanleiding tot de beslissing om zowel HTML pagina's als PDF documenten te scrapen.\\
\section{Natural Language Processing}
Natural Language Processing (NLP) is een tak van de informatietechnologie die instaat voor het omzetten van gesproken of geschreven menselijke taal in gestructureerde en verwerkbare data \autocite{Fanni2023}.
Het heeft als doel om een machine tekst te laten lezen met volledig begrip van alle complexiteit die eigen is aan de menselijke taal.\\
NLP is een verzamelnaam die meerdere toepassingen heeft waaronder het maken van samenvattingen, de classificatie van tekst, en het begrijpen van de boodschap \autocite{Khurana2022}. NLP is bijzonder populair als voorverwerking van AI toepassingen omdat het de ruwe taal omzet in betekenisvolle data waarmee modellen getraind worden.\\
Het bepalen van de relevantie van een tekst kan alleen maar nadat de betekenis ervan begrepen wordt. Een courante methode daarvoor is de ``term frequency-inverse document frequency'' \autocite{Havrlant2017}. Deze methode berekent een score die aangeeft hoe representatief een trefwoord is voor een bepaalde tekst. 
\section{Linked data}
Alle academische publicaties worden wereldwijd beheerd door Crossref aan de hand van een metadata record en een Digital Object Identifier (DOI) \autocite{Hendricks2020}. Crossref vertegenwoordigt de gemeenschap van academische tijdschriften en streeft voortdurend technologische innovatie na teneinde het beheer van wetenschappelijke publicaties te verbeteren.
In een dergelijk systeem, moeten de afzonderlijke entiteiten blijvend, betrouwbaar en onderscheidbaar geïdentificeerd kunnen worden. \textcite{Chandrakar2006} legt uit hoe de DOI voldoet aan deze criteria en de standaard identificatie is geworden van intellectuele eigendom op het internet.
\section{Semantic Search}
Semantic search interpreteert de betekenis van de query om aan de hand daarvan als resultaat een tekst te geven waarvan de betekenis het dichtst aanleunt bij de query, in plaats van een exacte match tussen de tekst en de query \autocite{Bast2016}.\\
Daarvoor wordt de tekst omgezet in een numerieke waarde, een vector, die een embedding genoemd wordt
\autocite{Almeida2019}. De embeddings zelf worden berekend aan de hand van een taalmodel (vb. Word2vec, GloVe, ..). Een taalmodel is op zijn beurt gemaakt door een zeer grote verzameling teksten te trainen met machine learning technieken. Aan de hand van deze embeddings kan het systeem dan de gelijkenis beoordelen tussen de query en de data van alle teksten.\\
Met semantic search verbetert het gevonden resultaat omdat het systeem de betekenis van de query begrijpt, zelfs in het geval van kleine afwijkingen in de tekst (vb. afkortingen, schrijffouten, ..). 




