\chapter{\IfLanguageName{dutch}{Stand van zaken}{State of the art}}%
\label{ch:stand-van-zaken}

% Tip: Begin elk hoofdstuk met een paragraaf inleiding die beschrijft hoe
% dit hoofdstuk past binnen het geheel van de bachelorproef. Geef in het
% bijzonder aan wat de link is met het vorige en volgende hoofdstuk.

% Pas na deze inleidende paragraaf komt de eerste sectiehoofding.
\section{IMIS}
Het Integrated Marine Information System (IMIS) \autocite{Haspeslagh2024} heeft als doel het mariene onderzoek in Vlaanderen te coördineren. Die rol wordt vervuld door op te treden als centraal kenniscentrum voor en door de mariene sector in Vlaanderen. De taken van IMIS gaan verder dan louter het uitbouwen van collecties met publicaties, ze omvatten ook referenties naar wetenschappers en naar projecten.
\section{Google Scholar}
Google Scholar (GS) is een online index van academische literatuur. De beta versie verscheen in 2004 en sindsdien wordt het systeem voornamelijk door academici gebruikt om een persoonlijke bibliotheek aan te leggen tezamen met statistieken omtrent citaties en h-indexen \footnote{De h-index van een wetenschappelijk onderzoeker komt overeen met de grootste h van het aantal publicaties die minstens h keer geciteerd zijn in ander werk. vb. Een h-index van 3 betekent dat de auteur minstens 3 publicaties heeft die elk minstens 3 keer geciteerd werden.}. GS laat toe om te gaan zoeken op sleutelwoorden, titel, auteur, domein en combinaties van deze \autocite{Noruzi2005}.\\
Er zijn verschillende kenmerken die bijdragen aan het succes van GS ten opzichte van de andere grote collecties. GS is beschikbaar zonder kosten, het bevat de grootste bibliografische collectie ter wereld, en de data is afkomstig van zowel publieke als niet publieke bronnen \autocite{Aguillo2011}.
\section{Web scraping}
Web scraping of Web crawling is een verzamelnaam voor een groep technieken waarmee gegevens ontrokken kunnen worden van webpagina's \autocite{Bhatt2023}. Web scrapers kunnen gebaseerd zijn op verschillende technologieën zoals 'spidering' en 'pattern matching'.\textcite{Lotfi2021} onderscheidt web scrapers zowel op basis van hun toepassing als op basis van hun  methodiek. Daarnaast biedt de programmeertaal waarin ze geïmplenteerd zijn ook telkens andere mogelijkheden \autocite{Bhatt2023}.
Python lijkt de voor de hand liggende taal voor web scraping door de beschikbaarheid van een groot aantal libraries voor het beheren van data \autocite{Kumar2023}.\\
Met de opkomst van AI blijkt het ook mogelijk om een webpagina te scrapen met gebruik van een Large Language Model (LLM) \autocite{Ahluwalia2024}. AI biedt voordelen voor het omgaan met dynamische websites in vergelijking met de klassieke methodes en verbetert de efficiëntie van het scrapen \autocite{Ayuso2024}.\\
Web scraping vergroot de snelheid en het volume van de data die verwerkt kan worden aanzienlijk, en vermindert ook het aantal fouten die zouden optreden door menselijke verwerking \autocite{Bhatt2023}. Uiteindelijk zet web scraping de online informatie om in business intelligence afhankelijk van het uitgangspunt van de eindgebruiker.\\
\textcite{Pratiba2018},\textcite{Rafsanjani2022},\textcite{Amin2024},\textcite{Sulistya2024} scrapen onafhankelijk van elkaar GS aan de hand van klassieke methodes.\\
Voorbeelden die daarvoor AI gebruiken, hebben nog niet tot een publicatie geleid, maar blijven beperkt tot blog posts (\autocite{Serpapiai2025}, \autocite{Anthropic2025}).\\
\textcite{Yang2017} beschrijft welke HTML elementen belangrijk zijn bij het ontleden van een GS pagina en \textcite{Rahmatulloh2020} toont hoe deze kunnen gemapt worden naar de custom code van de scraper.\\
Los van de manier waarop, blijft het doel steevast de omzetting van ongestructureerde gegevens op webpagina's naar gestructureerde data in een databank \autocite{Singrodia2019}.\\
\textcite{Mitchell2015} reikt een aantal methodes aan waarop dit gedaan kan worden, steeds afhankelijk van het beoogde resultaat. De skills om al die data te beheren en ermee te interageren is zo mogelijk nog belangrijk dan het scrapen op zich. Gescrapete data is niet gekenmerkt door strikte relaties, maar stelt eerder een collectie semi gestructureerde data voor. Een document store database lijkt in dit geval het meest aangewezen. \textcite{Lourenco2015} vergelijkt de gangbare NoSQL databases en stelt MongoDB voor als een consistente \footnote{alle clients zien steeds dezelfde data} document store database.\\
Bij web scraping wordt er constant met data van anderen gewerkt. Is dat wel legaal? Volgens \textcite{EPSI2015} is daar geen straightforward antwoord op, de situatie is afhankelijk van het betrokken land. Maar over het algemeen zijn de meest reguleringen wel in het voordeel van web scraping. Men moet vooral opletten met het respecteren van het eigendomsrecht wanneer inhoud verworven wordt door scraping en vervolgens verder gebruikt wordt.\\
Webpagina's bestaan voornamelijk uit HTM, maar daarnaast zijn ongeveer 70\% van de feiten die op het internet gepresenteerd worden, verkregen uit PDF-documenten \autocite{Singrodia2019}. Dit rechtvaardigt de keuze om zowel HTML pagina's als PDF documenten te scrapen.\\
\section{Natural Language Processing}
Natural Language Processing (NLP) is een tak van de informatietechnologie die instaat voor het omzetten van menselijke taal (gesproken of geschreven) in gestructureerde en verwerkbare data \autocite{Fanni2023}.
Het heeft als doel om een machine tekst te laten lezen met volledig begrip van alle dubbelzinnigheden.\\
NLP is een verzamelnaam die meerdere toepassingen heeft waaronder het maken van samenvattingen, de classificatie van tekst, en het begrijpen van de boodschap \autocite{Khurana2022}.\\
Het begrijpen van een tekst kan onder andere door het vinden van het trefwoord dat als beste het onderwerp beschrijft. Een courante methode daarvoor is de ``term frequency-inverse document frequency'' \autocite{Havrlant2017}.
\section{Linked data}
De academische publicaties worden duurzaam beheerd door Crossref aan de hand van een metadata record en een Digital Object Identifier (DOI) \autocite{Hendricks2020}. Crossref werkt voor en door alle geregistreerde uitgevers en streeft voortdurend technologische innovatie na teneinde het publicatieprocess van wetenschappelijke artikels te verbeteren.
In een dergelijk systeem, moeten de afzonderlijke entiteiten blijvend, betrouwbaar en onderscheidbaar geïdentificeerd kunnen worden. \textcite{Chandrakar2006} legt uit hoe de DOI voldoet aan deze criteria en de standaard identificatie is geworden van intellectuele eigendom op het internet.
\section{Semantic Search}
Semantic search interpreteert de betekenis van de query om aan de hand daarvan een resultaat te geven waarvan de betekenis het dichts aanleunt bij de query, in plaats van een exacte match tussen de tekst en de query \autocite{Bast2016}.\\
Daarvoor wordt de tekst omgezet in een numerieke waarde, een vector, die een embedding genoemd wordt
\autocite{Almeida2019}. Aan de hand van deze embeddings kan het systeem dan de gelijkenis vergelijken tussen de query en de data.\\
Het grote voordeel is dat de relevantie van het resultaat verbetert omdat het systeem de betekenis van de query begrijpt, zelfs in het geval van kleine afwijkingen ten opzichte van de data. 




