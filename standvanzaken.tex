\chapter{\IfLanguageName{dutch}{Stand van zaken}{State of the art}}%
\label{ch:stand-van-zaken}

% Tip: Begin elk hoofdstuk met een paragraaf inleiding die beschrijft hoe
% dit hoofdstuk past binnen het geheel van de bachelorproef. Geef in het
% bijzonder aan wat de link is met het vorige en volgende hoofdstuk.

% Pas na deze inleidende paragraaf komt de eerste sectiehoofding.
Wereldwijde onuitputtelijke digitale indexen (waaronder Google Scholar), diep maatschappelijk verankerde sociale media, het Internet Of Things, de toenemende belangstelling in AI, zorgen tezamen voor een gigantische hoeveelheid aan online data. Met deze grote berg van informatie die beschikbaar is op het world wide web, is het bijzonder relevant voor organisaties om te begrijpen hoe ze deze data kunnnen ontginnen teneinde er bruikbare informatie uit te filteren \autocite{Lotfi2021}. Een belangrijk begrip dat daarbij prominent op de voorgrond treedt, is 'web scraping' of 'web crawling'. <<Web scraping is een geautomatiseerd data extractie proces van websites met gebruik van gespecialiseerde software>> \autocite{Bhatt2023}. Maar web scraping is lang niet de enige techniek die gebruikt wordt om data van een webpagina te halen \autocite{Gray2012}. Andere technieken zijn het gebruik van API's, screen readers en het ontleden van online PDF documenten. Het voordeel van web scraping is dat de website niet over een API met ruwe data hoeft te beschikken. De HTML code waarmee de website opgebouwd is, wordt gebruikt door de scraper om de eigenlijke inhoud eruit te filteren.\\
Web scrapers kunnen gebaseerd zijn op verschillende technologieën zoals 'spidering' en 'pattern matching'. Daarnaast biedt de programmeertaal waarin ze geïmplenteerd zijn ook telkens andere mogelijkheden \autocite{Bhatt2023}.
\textcite{Lotfi2021} onderscheidt web scrapers zowel op basis van hun toepassing (vb. medisch, social media, financieel, marketing, onderzoek) als op basis van hun  methodiek (vb. copy \& paste, HTML parsing, DOM parsing, HTML DOM, reguliere expressies, XPath, vertical aggregation platform, semantic annotation recognizing, computer vision web page analyzer). Hoewel de methodologie kan wijzigen, blijft het uitgangspunt van integriteit, correctheid en betrouwbaarheid van de data steeds van primordiaal belang \autocite{Lotfi2021}.
Verder duidt \textcite{Lotfi2021} ook nog op het iteratieve karakter van een web scaper. Het programma wordt gevoed met een url, en zal dan op zijn beurt nieuwe urls zoeken op de pagina zodat die ook bezocht kunnen worden.\\
\textcite{Singrodia2019} vult bovenstaande kenmerken van web scraping nog verder aan met de systematische omzetting van ongestructureerde gegevens op webpagina's naar gestructureerde data in een databank. De gegenereerde data heeft aanleiding tot filtering of tot statistieken. Het biedt vele voordelen aangezien de data opgeschoond is en daardoor als vrij van fouten kan beschouwd worden. Andere voordelen zijn tijdswinst en centrale opslag wat verdere verwerking ten goede komt.\\
Daarenboven vergroot web scraping de snelheid en het volume van de data die verwerkt kan worden aanzienlijk, en vermindert het ook het aantal fouten die zouden optreden door menselijke verwerking \autocite{Bhatt2023}. Uiteindelijk zet web scraping de online informatie om in business intelligence afhankelijk van het uitgangspunt van de eindgebruiker.\\
Na web scrapers besproken te hebben, is het niet onbelangrijk even stil te staan bij de vraag of web scrapen wel legaal is? Volgens \textcite{EPSI2015} is daar geen straightforward antwoord op, de situatie is afhankelijk van het betrokken land. Maar over het algemeen zijn de meest reguleringen wel in het voordeel van web scraping. Men moet vooral opletten met het respecteren van het eigendomsrecht wanneer inhoud verworven wordt door scaping en vervolgens verder gebruikt wordt.\\
Tot slot van de stand van zake omtrent web scraping nog een pleidooi voor Python. Al is het perfect mogelijk om web scraping te ontwikkelen in Php, Java, en andere, toch biedt Python de meest gebruiksvriendelijke tools aan volgens \textcite{Kumar2023}. <<Beautiful Soup is een van de eenvoudigste bibliotheken om data te scrapen van websites. Een simpele find\_all() in Beautiful Soup, is krachtig genoeg om de data uit het gehele document te doorzoeken. Daarna is de taak om de data te structureren. Dat kan in Python aan de hand van Pandas die in staat is om de data in een geordend formaat te presenteren.>>\\
Om verder gebruik te maken van deze data moet deze eerst gestuctureerd opgeslaan worden. \textcite{Mitchell2015} reikt een aantal methodes aan waarop dit gedaan kan worden, steeds afhankelijk van het beoogde resultaat. De skills om al die data te beheren en ermee te interageren is zo mogelijk nog belangrijk dan het scrapen op zich. De data waarvan sprake is niet gekenmerkt door strikte relaties, maar stelt eerder een collectie semi gestructureerde data voor. Een document store database lijkt in dit geval het meest aangewezen. \textcite{Lourenco2015} vergelijkt de gangbare NoSQL databases en stelt MongoDB voor als een consistente \footnote{alle clients zien steeds dezelfde data} document store database.\\
De website in de spotlight voor dit onderzoek is Google Scholar (GS), de grootste bron van wetenschappelijke publicaties op heden. De beta versie verscheen in 2004 en sindsdien wordt het systeem voornamelijk door academici gebruikt om een persoonlijke bibliotheek aan te leggen tezamen met statistieken omtrent citaties en h-indexen \footnote{De h-index van een wetenschappelijk onderzoeker komt overeen met de grootste h van het aantal publicaties die minstens h keer geciteerd zijn in ander werk.}. Volgens \textcite{Oh2019} is er een symbiose tussen GS and de academische wereld waarbij academici gratis hun werk kunnen aanbieden op GS. GS op zijn beurt zorgt voor goede visibiliteit van dat werk door onder andere citaten, referenties en een oplijsting van gelijkaardige onderwerpen. In tegenstelling tot de baanbrekende vernieuwing die GS introduceerde, gaat het raadplegen van deze gegevens wel gepaard met een aantal hindernissen, in het bijzonder op grote schaal.\\
Web scraping is 1 van de technieken die aangewend worden om dit probleem te omzeilen. Meerdere studies (\autocite{Pratiba2018},\autocite{Rafsanjani2022},\autocite{Amin2024},\autocite{Sulistya2024}) gebruiken bovenstaande concepten van web scrapers en gestructureerde opslag om automatisch data te ontleden van GS aan de hand van gebruiksvriendelijke interfaces. Er zijn onderling steeds wel verschillen tussen de gebruikte tools en het gekozen formaat die beide afhankelijk zijn van het beoogde resultaat. Maar het omliggend kader is steeds hetzelfde met gebruik van web scraping en gestructureerde opslag.\\
Het is evident dat HTML pagina's verwerkt kunnen worden door web scrapers, maar daarnaast zijn ongeveer 70\% van de feiten die op het internet gepresenteerd worden, verkregen uit PDF-documenten \autocite{Singrodia2019}. Dit verklaart de noodzaak om zowel HTML pagina's als PDF documenten te scrapen.\\
\textcite{Yang2017} beschrijft welke HTML elementen belangrijk zijn bij het ontleden van een GS pagina en \textcite{Rahmatulloh2020} toont hoe deze kunnen gemapt worden naar de custom code van de scraper.\\
