%%=============================================================================
%% Methodologie
%%=============================================================================

\chapter{\IfLanguageName{dutch}{Web scraping}{Web scraping}}%
\label{ch:web_scraping}

%% TODO: In dit hoofstuk geef je een korte toelichting over hoe je te werk bent
%% gegaan. Verdeel je onderzoek in grote fasen, en licht in elke fase toe wat
%% de doelstelling was, welke deliverables daar uit gekomen zijn, en welke
%% onderzoeksmethoden je daarbij toegepast hebt. Verantwoord waarom je
%% op deze manier te werk gegaan bent.
%% 
%% Voorbeelden van zulke fasen zijn: literatuurstudie, opstellen van een
%% requirements-analyse, opstellen long-list (bij vergelijkende studie),
%% selectie van geschikte tools (bij vergelijkende studie, "short-list"),
%% opzetten testopstelling/PoC, uitvoeren testen en verzamelen
%% van resultaten, analyse van resultaten, ...
%%
%% !!!!! LET OP !!!!!
%%
%% Het is uitdrukkelijk NIET de bedoeling dat je het grootste deel van de corpus
%% van je bachelorproef in dit hoofstuk verwerkt! Dit hoofdstuk is eerder een
%% kort overzicht van je plan van aanpak.
%%
%% Maak voor elke fase (behalve het literatuuronderzoek) een NIEUW HOOFDSTUK aan
%% en geef het een gepaste titel.
\section{Inleiding}
De e-mails afkomstig van Google Scholar bevatten een lijst met zoekresultaten. Het formaat van de e-mail is HTML en de opmaak van de lijst is gelijkaardig aan die van de Google Scholar resultaten pagina. Dit wordt algemeen benoemd als een SERP en is voor iedereen die vertrouwd is met het internet herkenbaar als de lijst met zoekresultaten van Google (zie figuur \ref{fig:Google_Scholar_SERP}).
\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{./4_NLP/SERP.jpg}
    \caption[Google Scholar SERP.]{\label{fig:Google_Scholar_SERP}Google Scholar SERP.}
\end{figure}\\
De Google Scholar SERP heeft een vaste structuur, namelijk een lijst met zoekresultaten bestaande uit:
\begin{itemize}
    \item titel
    \item link naar de webpagina van de publicatie
    \item auteurs
    \item naam van het tijdschrift
    \item jaartal
    \item abstract of fragment van het abstract
\end{itemize} 
Bovenstaande gegevens moeten uit de SERP gefilterd worden zodat ze opgeslaan kunnen worden voor verder gebruik in de volgende stappen.\\
Informatie uit HTML pagina's halen is algemeen gekend onder de naam ``web scraping'' of  ``web crawling''. Deze techniek geniet momenteel veel aandacht omdat hij de gebruiker in staat stelt om veel data te verzamelen. Die data dient dan weer als brandstof voor AI. Het punt is dat in die context meerdere technieken bestaan om dezelfde job toe doen:
\begin{itemize}
    \item web scraping met gebruik van een LLM \footnote{Large Language Model}
    \begin{itemize}
        \item online model
        \item lokaal model
    \end{itemize}
    \item web scraping met verwerking van de DOM
    \begin{itemize}
        \item Beautiful Soup
        \item SerpAPI
    \end{itemize}
\end{itemize} 

\section{Web scraping met gebruik van een LLM }
De meest gekende vorm van web scraping gebruikt de DOM structuur om er de inhoud uit te filteren. Daar komt dus niets van AI bij kijken. Het probleem met die aanpak is dat de custom code die de HTML structuur verwerkt, sterk afhankelijk is van de HTML zelf. Bijvoorbeeld wanneer de titel van een publicatie tussen <h3> tags staat die gekenmerkt worden door een class="gse\_alrt\_title" (zie codefragment \ref{code:HTMLcodefragment}), dan zal de custom code specifiek filteren op die DOM elementen.
\begin{listing}
<h3 style=\"font-weight:normal;margin:0;font-size:17px;line-height:20px;\"><span style=\"font-size:11px;font-weight:bold;color:#1a0dab;vertical-align:2px\">[HTML]</span> <a href="https://scholar.google.be/scholar\_url?url=https://www.sciencedirect.com/science/article/pii/S0191886925001308\&amp;hl=nl\&amp;sa=X\&amp;d=9286058128011819102\&amp;ei=9W3gZ7v-GcCSieoPqLGJyAM\&amp;scisig=AFWwaebeqxEetR78Fz7JpxPtT7ui\&amp;oi=scholaralrt\&amp;hist=P\_QG1LwAAAAJ:2727769339669043622:AFWwaeYuVCLO-kY6yEQWAvLJNk68\&amp;html=\&amp;pos=0\&amp;folt=kw-top\" class=\"gse\_alrt\_title\" style=\"font-size:17px;color:#1a0dab;line-height:22px\">The role of narcissistic personality <b>traits </b>in bullying behavior in adolescence–A systematic review and meta-analysis</a></h3>
\caption[Prompt htmlfragment]{HTML fragment van de titel van een publicatie.}
\label{code:HTMLcodefragment}
\end{listing}
Maar wanneer de structuur van de HTML wijzigt om welke reden dan ook, dan zal de web scraper niet langer werken zolang de custom code niet werd aangepast.\\
Daarom begint dit hoofdstuk met meer recente technieken die gebaseerd zijn op AI en die niet strikt afhankelijk zijn van de DOM structuur.
\subsection{Web scraping met OpenAI}
LLMs zijn bijzonder goed in het beantwoorden van vragen. OpenAI is voor het brede publiek beter gekend door zijn chatbot ChatGPT. Maar hoe goed is OpenAI in het parsen van een webpagina? OpenAI biedt ook een API \autocite{Openaideveloperplatform2025} aan waarmee gebruikers opdrachten kunnen sturen naar een model. De opdracht/vraag in kwestie is: ``Geef de titels, orginele links, auteurs, naam van de tijdschriften, jaartal van de publicaties en tekstfragmenten van hetvolgende Google Scholar zoekresultaat.''\\
Het online artikel \textcite{Serpapiai2025} beschrijft stap voor stap hoe de gevraagde gegevens verkregen kunnen worden met het ``gpt-4-1106-preview'' model van OpenAI.
De code op github \textcite{Depaepeopenai2025} maakt hiervan een implementatie aangepast voor de Google Scholar alerts.\\
De HTML wordt eerst ontdaan van overbodige tags zodat alleen de inhoud behouden blijft zoals te zien is in codefragment \ref{code:Cleaningcodefragment}
\begin{listing}
    \begin{minted}{python}
        # Remove all occurrences of content between <head> and </head>
        body_text = re.sub(r'<head.*?>.*?</head>', '', body_text, flags=re.DOTALL)
        # Remove all occurrences of content between <script> and </script>
        body_text = re.sub(r'<script.*?>.*?</script>', '', body_text, flags=re.DOTALL)
        # Remove all occurrences of content between <style> and </style>
        body_text = re.sub(r'<style.*?>.*?</style>', '', body_text, flags=re.DOTALL)
    \end{minted}
    \caption[Cleaning codefragment]{Codefragment voor het opkuisen van de HTML.}
    \label{code:Cleaningcodefragment}
\end{listing}
De prompt voor het model is te zien in codefragment \ref{code:Promptcodefragment}.
\begin{listing}
    \begin{minted}{python}
        messages=[
        {"role": "system",
            "content": "You are a master at scraping Google Scholar results data. Scrape top 10 organic results data from Google Scholar search result page."},
        {"role": "user", "content": body_text}
        ],
    \end{minted}
    \caption[Prompt codefragment]{Codefragment voor het opstellen van een prompt.}
    \label{code:Promptcodefragment}
\end{listing}
De verwachte parameters die het model moet zoeken zijn te zien in codefragment \ref{code:Parse_opties_codefragment}.\\
\begin{listing}
    \begin{minted}{python}
        "function": {
            "name": "parse_data",
            "description": "Parse organic results from Google Scholar SERP raw HTML data nicely",
            "parameters": {
                'type': 'object',
                'properties': {
                    'data': {
                        'type': 'array',
                        'items': {
                            'type': 'object',
                            'properties': {
                                'title': {'type': 'string'},
                                'original_url': {'type': 'string'},
                                'authors': {'type': 'string'},
                                'year_of_publication': {'type': 'integer'},
                                'journal_name': {'type': 'string'},
                                'snippet': {'type': 'string'}
                            }
                        }
                    }
                }
            }
        }
    \end{minted}
    \caption[Parse opties codefragment]{Codefragment voor het opstellen van de zoekopties.}
    \label{code:Parse_opties_codefragment}
\end{listing}
Het is vereist om een account te registreren bij OpenAI en om dit account te crediteren. Vervolgens kan er met de API gewerkt worden en wordt er betaald naargelang het verbruik.
Een test met 1 Google Scholar alert met 10 zoekresultaten heeft een prijs van 0,17\$ zoals te zien is in figuur \ref{fig:OpenAI_dashboard}. Het model gebruikte daarvoor 14743 tokens. 
\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{./4_NLP/openai_billing.png}
    \caption[OpenAI dashboard.]{\label{fig:OpenAI_dashboard}OpenAI dashboard.}
\end{figure}
De test was succesvol. Het model vond de gevraagde parameters voor elk van de 10 zoekresultaten. Toch houdt het onderzoek naar web scraping hier niet op. Het resultaat tot zover is immers betalend en dat is niet noodzakelijk de beste oplossing voor het probleem. Bovendien werkt deze oplossing enkel met OpenAI modellen wat weinig flexibiliteit toelaat.

\subsection{Web scraping met Mirascope en Anthropic}
De vorige scraper was zeer specifiek geschreven voor OpenAI. De code zou niet werken voor een model van een andere provider. Daarom wordt er verder gezocht naar een meer generieke methode die niet afhankelijk is van de provider van het model.\\
Mirascope \textcite{Mirascope2025} is een gespecialiseerde code library om op uniforme wijze met verschillende LLMs te werken.
Het online artikel \textcite{Anthropic2025} beschrijft stap voor stap hoe de gevraagde gegevens verkregen kunnen worden aan de hand van Mirascope en het Anthropic Claude model. Net zoals OpenAI biedt Anthropic ook een API \autocite{Anthropicmodel2025} aan voor development. Er moet eveneens een account geregistreerd worden bij Anthropic maar hun free tier is uitgebreider dan dat van OpenAI zodat het account voor dit experiment niet gecrediteerd moest worden.\\
De code op github \textcite{Depaepeanthropic2025} maakt hiervan een implementatie aangepast voor de Google Scholar alerts.
De parameters die opgezocht moeten worden, dienen eerst als velden en schema gedeclareerd te worden. Dit is te zien in codefragmenten \ref{code:Field} en \ref{code:Schema}.
\begin{listing}
    \begin{minted}{python}
        class FieldDefinition(BaseModel):
        """Define the fields to extract from the webpage."""
        
        name: str = Field(..., description="The desired name for this field.")
        type: Literal["str", "int", "float", "bool", "list"]
    \end{minted}
    \caption[Field codefragment]{Codefragment voor het opstellen van een Pydantic Field.}
    \label{code:Field}
\end{listing}
\begin{listing}
    \begin{minted}{python}
        class SchemaGenerator(AnthropicExtractor[list[FieldDefinition]]):
        """Generate a schema based on a user query."""
        
        api_key = settings.anthropic_api_key
        
        extract_schema: Type[list] = list[FieldDefinition]
        
        prompt_template = """
        Call your tool with field definitions based on this query:
        {query}
        """
        
        query: str
    \end{minted}
    \caption[Schema codefragment]{Codefragment voor het opstellen van een Pydantic Schema.}
    \label{code:Schema}
\end{listing}
Met deze structuur van veld en schema kan vervolgens voor elke opdracht een gepast model gemaakt worden aan de hand van de code in codefragment \ref{code:Model}
\begin{listing}
    \begin{minted}{python}
            def generate_schema(self) -> None:
        """Sets `extract_schema` to a schema generated based on `query`."""
        field_definitions = SchemaGenerator(query=self.query).extract()
        model = create_model(
        "ExtractedFields",
        __doc__=DEFAULT_TOOL_DOCSTRING,
        **{
            field.name.replace(" ", "_"): (field.type, ...)
            for field in field_definitions
        },
        )
        self.extract_schema = list[model]
    \end{minted}
    \caption[Model codefragment]{Codefragment voor het opstellen van een Pydantic Model.}
    \label{code:Model}
\end{listing}
Dan kan er geparsed worden met de resultaten volgens het gevraagde schema zoals getoond wordt in codefragment \ref{code:Extract}.
\begin{listing}
    \begin{minted}{python}
        @computed_field
        @property
        def webpage_content(self) -> str:
        """Returns the text content of the webpage found at `url`."""
        soup = BeautifulSoup(self.html_text, "html.parser", from_encoding="utf-8")
        text = soup.get_text()
        for link in soup.find_all("a"):
        text += f"\n{link.get('href')}"
        return text
    \end{minted}
    \caption[Extract codefragment]{Codefragment voor het parsen.}
    \label{code:Extract}
\end{listing}
\\
Ook deze test was succesvol. Het model vond de gevraagde parameters voor elk van de 10 zoekresultaten. Toch houdt het onderzoek naar web scraping ook hier niet op. De oplossing tot zover heeft nog steeds een beperkte free tier en meerdere opdrachten zouden ook aanleiding geven tot een betalende service.
Daarnaast zijn tot zover beide oplossingen ook afhankelijk van een online service wat in bepaalde gevallen niet wenselijk kan zijn (bijvoorbeeld wanneer de klant een on-premise oplossing wil).
\subsection{Web scraping met een lokaal model}
Om niet afhankelijk te zijn van een online service moet het model lokaal gehost worden. \textcite{Ollama2025} is een platform om LLMs te hosten op je eigen systeem. Eenmaal geïnstalleerd (bijvoorbeeld via Docker) kan een model naar keuze gedownload en uitgevoerd worden op de lokale machine.
Het is daarbij aan de gebruiker om te kiezen welk model het best geschikt is voor het probleem, maar ook om de vereisten van het model en de beschikbare hardware op elkaar af te stemmen. 
\begin{itemize}
    \item LLaMA — geschikt voor een brede waaier aan toepassingen, LLaMA blinkt uit in het begrijpen en het genereren van tekst die de menselijke taal nabootst.
    \item Gemma — biedt een lichtere versie van een model dat ook geschikt is voor web scraping.
\end{itemize}
De commando's in codefragment \ref{Ollama} illustreren hoe het llama3:8b model (4.7GB) gedownload en uitgevoerd wordt op een lokaal systeem via Docker.
\begin{listing}
    docker exec -it ollama ollama pull llama3:8b
    docker exec -it ollama ollama run llama3
    \caption[Ollama]Ollama}
    \label{Ollama}
\end{listing}
Het online artikel \textcite{Scrapegraphaillama2025} beschrijft stap voor stap hoe webpagina's gescraped kunnen worden aan de hand van ScrapegraphAI en het llama3 model.
\textcite{ScrapeGraphAI2025} is een library die LLMs en grafen combineert om webpagina's te scrapen. \textcite{Llama32025} is ideaal voor breed gebruik en is kosten-efficiënt. Het munt uit in het begrijpen en genereren van menselijke teksten.
De code op github \textcite{Depaepeollama2025} maakt hiervan een implementatie aangepast voor de Google Scholar alerts. De code veronderstelt dat ollama draait op het systeem en dat het llama3 model erop uitgevoerd wordt.
De test was niet succesvol. Het model vond geen enkele van de gevraagde parameters voor geen enkele van de 10 zoekresultaten.
In eerste instantie werd gedacht dat het geselecteerde model te zwak is voor deze taak. Daarom werd een andere test uitgevoerd op een systeem met 40GB RAM met het gemma3:27b model (17GB) \autocite{Gemma32025}. Maar ook deze test was niet succesvol: de opdracht bleef gedurende 1 uur lopen zonder resultaat, waarna het commando manueel onderbroken werd. 
\section{Web scraping via het parsen van de DOM}
Tot zover is er nog geen bruikbaar resultaat. Het gebruik van een online model is betalend en kan niet gewenst zijn door de klant. Anderzijds leidt een lokaal model voorlopig niet tot een goed resultaat. Daarom wordt er teruggegrepen naar de meer klassieke benadering van web scraping waarbij de HTML structuur ontleed wordt aan de hand van custom code.
De Google Scholar SERP is een gestructureerde HTML pagina. Voor het parsen wordt er gebruik gemaakt van deze structuur.
Elk afzonderlijk resultaat heeft een H3-tag met als klasse 'gse\_alrt\_title' waarin de titel staat. De korte tekst van het zoekresultaat staat in een DIV-tag met als klasse 'gse\_alrt\_sni'. Tussen de titel en het snippet staan de auteurs, de uitgever en het jaartal gegroepeerd in een DIV-tag zoals te zien is in figuur \ref{fig:serp-html}.
\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{./2_parse_zoekresultaat/serp-html.JPG}
    \caption[HTML structuur van de GS alert.]{\label{fig:serp-html}HTML structuur van de GS alert.}
\end{figure}
\FloatBarrier
Meerdere specifieke libraries laten toe om HTML en XML te parsen, maar de meest gekende bibliotheek is Beautiful Soup \autocite{Soup2025}. Door de naam van de HTML-tag en de gebruikte klasse (gse\_alrt\_title, en gse\_alrt\_sni) mee te geven aan Beautiful Soup, worden de overeenkomstige elementen in het HTML-document weergegeven. Beautiful Soup laat ook toe om te navigeren in het document zodat het element met de auteurs, de uitgever en het jaartal gemakkelijk gevonden kan worden.
De code op github \textcite{DepaepeBeautifulsoup2025} maakt hiervan een implementatie aangepast voor de Google Scholar alerts. De volledige uitwerking is te zien in bijlage Parse Google Scholar zoekopdracht.
De test met een SERP met 10 zoekresultaten was geslaagd. De gevraagde parameters worden gevonden voor elk van de 10 resultaten.\\
Tenslotte moet ook nog SerpAPI \autocite{Serpapi2025} vermeld worden. Tijdens het onderzoek naar web scraping kwam dit platform meermaals naar voren, zelfs voor de toepassingen die gebruik maken van AI. SerpAPI biedt specifieke oplossingen voor elke mogelijke search engine, waaronder ook Google Scholar. Alleen focust hun product telkens op de online search engine en niet op alerts waardoor de zoekresultaten niet incrementeel zijn.\\
Tot besluit van dit hoofdstuk wordt er dus verder gewerkt met het parsen van de zoekresultaten aan de hand van Beautiful Soup. Omdat deze methode volledig afhankelijk is van de structuur van de HTML, is ze zeer begrijpbaar. De HTML wordt omgezet in Beautiful Soup en vervolgens kunnen de gewenste tags en klasses rechtstreeks geselecteerd worden.\\
Om aan de tekortkoming van de grote mate van afhankelijkheid tegemoet te komen, kan er een notificatie geïmplementeerd worden die een bericht stuurt wanneer Beautiful Soup de gewenste tags in het HTML document niet langer vindt.
