%%=============================================================================
%% Methodologie
%%=============================================================================

\chapter{\IfLanguageName{dutch}{Natural Language Processing}{Natural Language Processing}}%
\label{ch:natural_language_processing}

%% TODO: In dit hoofstuk geef je een korte toelichting over hoe je te werk bent
%% gegaan. Verdeel je onderzoek in grote fasen, en licht in elke fase toe wat
%% de doelstelling was, welke deliverables daar uit gekomen zijn, en welke
%% onderzoeksmethoden je daarbij toegepast hebt. Verantwoord waarom je
%% op deze manier te werk gegaan bent.
%% 
%% Voorbeelden van zulke fasen zijn: literatuurstudie, opstellen van een
%% requirements-analyse, opstellen long-list (bij vergelijkende studie),
%% selectie van geschikte tools (bij vergelijkende studie, "short-list"),
%% opzetten testopstelling/PoC, uitvoeren testen en verzamelen
%% van resultaten, analyse van resultaten, ...
%%
%% !!!!! LET OP !!!!!
%%
%% Het is uitdrukkelijk NIET de bedoeling dat je het grootste deel van de corpus
%% van je bachelorproef in dit hoofstuk verwerkt! Dit hoofdstuk is eerder een
%% kort overzicht van je plan van aanpak.
%%
%% Maak voor elke fase (behalve het literatuuronderzoek) een NIEUW HOOFDSTUK aan
%% en geef het een gepaste titel.
\section{Inleiding}
Google Scholar heeft feitelijk al gezocht naar relevante publicaties voor onze zoekopdracht. De sortering van de zoekresultaten is op basis van de omvang van de integrale tekst, het aanzien van het tijdschrift, het aanzien van de auteurs en het aantal citaties (inclusief de leeftijd ervan). Op die manier volgt Google Scholar de manier van werken binnen de academische wereld.\\
Maar dit laat nog steeds ruimte om voor elk zoekresultaat een score af te leiden die aangeeft hoe relevant de publicatie is voor IMIS. De zoekopdracht voor het VLIZ bijvoorbeeld gebruikt als 1 van de trefwoorden ``Simon Stevin'', zijnde het wetenschappelijk vaartuig van het VLIZ. Dat betekent dat Google Scholar ook zoekresultaten zal geven van publicaties die verwijzen naar ``Simon Stevin'', de vlaamse geleerde uit de 16de eeuw. Het is interessant om die artikels te kunnen markeren met een lagere score ten opzichte van publicaties die het trefwoord ``VLIZ'' bevatten.\\
Natural Language Processing is een verzamelnaam van een hele groep technieken die tekst omzetten in informatie. De toepassingen en technieken zijn zeer uitgebreid en daarom ook onderverdeeld in verschillende deelgebieden. Het bepalen van de relevantie van een tekst in functie van een trefwoord valt eerder onder het deelgebied van de ``Natural Language Understanding''.\\

\section{Relevantiescore}
Het uitgangspunt is om een score te berekenen door te tellen hoe vaak een trefwoord voorkomt in een tekst. De frequentie is dan rechtevenredig met de relevantie van de tekst. Een eenvoudige techniek voor het bepalen van de frequentie is aan de hand van een ``Bag of Words'' (BoW). Dat is een tabel met een rij voor elke tekst en een kolom voor elk uniek woord. De cellen tonen het aantal keer dat het woord voorkomt in de tekst.  Dit is te zien in figuur \ref{fig:bow}.
\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{./4_NLP/bow.jpg}
    \caption[Bag of Words.]{\label{fig:bow}Bag of Words.}
\end{figure}
Toch is de frequentie op zich nog niet de beste waardemeter voor een teksts. Er kunnen namelijk woorden heel vaak voorkomen die op zich weinig vertellen over het onderwerp. Om daaraan tegemoet te komen bestaat er een variant van de BoW, die in plaats van gewoon te tellen de ``term frequency-inverse document frequency'' (Tf-Idf) geeft. De formules zijn te zien in codefragment \ref{code:tf-idf} en het resultaat in figuur \ref{fig:tf-idf}.
\begin{listing}
    \begin{equation}
        score\ =\ tf\ \ast\ idf
    \end{equation}  
    where
    \begin{equation}
        tf=term\ frequency\ \left(see\ above\right)
    \end{equation} 
    
    \begin{equation}
        idf_t=log\left(\frac{N}{df_t}\right)
    \end{equation}  
    
    \begin{equation} 
        N=total\ number\ ofdocuments
    \end{equation}  
    
    \begin{equation} 
        df_t=the\ number\ of\ documents\ in\ which\ term\ t\ occurs
    \end{equation}  
    \caption[term frequency-inverse document frequency]{term frequency-inverse document frequency}
    \label{code:tf-idf}
\end{listing}
\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{./4_NLP/tf-idf.jpg}
    \caption[term frequency-inverse document frequency.]{\label{fig:bow}term frequency-inverse document frequency.}
\end{figure}
Maar de relevantie van een publicatie kan niet afhangen van het aantal zoekresultaten, daarom valt de ``inverse document frequency'' weg. In de plaats zijn er andere parameters om de relevantie te benaderen.
\begin{itemize}
    \item De topic-sentence ratio: Een ratio van het trefwoord count ten opzichte van het totaal aantal zinnen.
    \item De topic-noun ratio: Een ratio van het trefwoord count ten opzichte van het totaal aantal zelfstandige naamwoorden.
\end{itemize}
Aan de hand van die parameters kan een relevantiescore berekend worden zoals te zien in codefragment \ref{code:relevantiescore}.
\begin{listing}
    \begin{equation}
        score=log\left(\ topic count\ \ast\ topic-sentence ratio\ \ast\ topic-noun ratio\right)
    \end{equation}  
    \caption[relevantiescore]{relevantiescore}
    \label{code:relevantiescore}
\end{listing}
De code op github \textcite{Depaepenlp2025} maakt hiervan een implementatie aangepast voor de Google Scholar alerts.
\section{Tekstverwerking}
De BoW geeft wel aanleiding voor tabellen met zeer grote dimensies. Hoe meer teksten er zijn, des te uitgebreider zal de woordenschat worden. Daarom wordt de BoW altijd voorafgegaan door tekstverwerking die stopwoorden \footnote{Stopwoorden (of, a, the, in ,you, ...) komen vaak voor maar hebben geen toegevoegde waarde over het onderwerp.} verwijdert en lemmatisering \footnote{Lemmatisering zet woorden om naar hun basisvorm, maar houdt daarbij rekening met de context. Voorbeeld ``caring'' wordt omgezet in ``care'' en niet in ``car''.}.\\
Voornaamwoorden kunnen verwijzen naar het trefwoord. Het is dus relevant om ze te vervangen door het trefwoord zelf zodat het van invloed is op de score. ``Coreference resolution'' is daar de geschikte NLP techniek voor. In een eerste fase worden alle gerelateerde voornaamwoorden en zelfstandige naamwoorden opgezocht. In een tweede fase worden alle gevonden voornaamwoorden vervangen door hun bijhorende zelfstandig naamwoord. Daarvoor gebruikt deze techniek een coreference model dat bestaat uit opeenvolgende paren van tokens met een bijhorende coreference score.
Het opzoeken van paren is te zien in codefragment \ref{code:coreference}.
\begin{listing}
    \begin{minted}{python}
        import spacy
        ...
            def __init__(self, db_service: DBService, logging_service: LoggingService):
        self.db_service = db_service
        self.logging_service = logging_service
        self.nlp = spacy.load("en_coreference_web_trf")
        ...
            def coreference_resolution(self, text):
        doc = self.nlp(text)
        spans = doc.spans
        span_array = []
        self.logging_service.logger.debug(spans)
        for spangroup in spans.values():
        span_tuple = []
        for span in spangroup:
        self.logging_service.logger.debug(text[span.start_char:span.end_char])
        span_tuple.append(text[span.start_char:span.end_char])
        span_array.append(span_tuple)
        return span_array
    \end{minted}
    \caption[Coreference resolver]{Coreference resolver}
    \label{code:coreference}
\end{listing}
